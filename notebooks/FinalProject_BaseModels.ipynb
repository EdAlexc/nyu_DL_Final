{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we find the base models that were tested for our final project, before altering top layers."
      ],
      "metadata": {
        "id": "X6Y2bjtgOUPb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vekc80G9L3Ex"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle transformers >> /dev/null\n",
        "!pip install transformers\n",
        "\n",
        "#installed twice as local instance was having issues with one or the other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXlVKTlpMYsm"
      },
      "outputs": [],
      "source": [
        "# UNCOMMENT IF: setting up connection to kaggle instance to import data\n",
        "# !rm -r ~/.kaggle | true && mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle competitions download -c feedback-prize-2021"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eePeHuvANVzV",
        "outputId": "fc503cee-29c1-4b41-eb19-b12977eb2900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78XoPdgdKyM9"
      },
      "outputs": [],
      "source": [
        "!unzip feedback-prize-2021.zip -d raw_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nm4R7eL6LRzq"
      },
      "outputs": [],
      "source": [
        "# from transformers import RobertaTokenizer, RobertaModel\n",
        "# import transformers\n",
        "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "# from transformers import BertModel\n",
        "# import torchtext\n",
        "# from torchtext.data import Dataset\n",
        "# from torchtext.legacy import data\n",
        "import transformers\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# send to device, and match with parallel connection\n",
        "def send_to_device(net):\n",
        "    net = net.to(device)\n",
        "    if device == 'cuda':\n",
        "        net = torch.nn.DataParallel(net)\n",
        "        cudnn.benchmark = True\n",
        "        print('\\t ==> Model sent to gpu...')\n",
        "    else:\n",
        "        print('\\t ==> Model sent to cpu...')\n",
        "        \n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0FhUfKvOic5"
      },
      "outputs": [],
      "source": [
        "sample = pd.read_csv('raw_data/sample_submission.csv')\n",
        "sample.loc[sample['class'].notnull()].head() # verifying that this sheet contains only the submission format required by kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-JBh6u_OtzF"
      },
      "outputs": [],
      "source": [
        "exp_dataset = pd.read_csv('raw_data/train.csv')\n",
        "exp_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIMCz7OeP5Iw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "1e6aa4de-fc89-4a6e-b6a2-72725fea95ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             id  discourse_id  discourse_start  discourse_end  \\\n",
              "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
              "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
              "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
              "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
              "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
              "\n",
              "                                      discourse_text discourse_type  \\\n",
              "0  Modern humans today are always on their phone....           Lead   \n",
              "1  They are some really bad consequences when stu...       Position   \n",
              "2  Some certain areas in the United States ban ph...       Evidence   \n",
              "3  When people have phones, they know about certa...       Evidence   \n",
              "4  Driving is one of the way how to get around. P...          Claim   \n",
              "\n",
              "  discourse_type_num                                   predictionstring  \n",
              "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
              "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
              "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
              "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
              "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e0d12c87-bc4d-4897-9767-f548dd50a46a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>discourse_id</th>\n",
              "      <th>discourse_start</th>\n",
              "      <th>discourse_end</th>\n",
              "      <th>discourse_text</th>\n",
              "      <th>discourse_type</th>\n",
              "      <th>discourse_type_num</th>\n",
              "      <th>predictionstring</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>423A1CA112E2</td>\n",
              "      <td>1.622628e+12</td>\n",
              "      <td>8.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>Modern humans today are always on their phone....</td>\n",
              "      <td>Lead</td>\n",
              "      <td>Lead 1</td>\n",
              "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>423A1CA112E2</td>\n",
              "      <td>1.622628e+12</td>\n",
              "      <td>230.0</td>\n",
              "      <td>312.0</td>\n",
              "      <td>They are some really bad consequences when stu...</td>\n",
              "      <td>Position</td>\n",
              "      <td>Position 1</td>\n",
              "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>423A1CA112E2</td>\n",
              "      <td>1.622628e+12</td>\n",
              "      <td>313.0</td>\n",
              "      <td>401.0</td>\n",
              "      <td>Some certain areas in the United States ban ph...</td>\n",
              "      <td>Evidence</td>\n",
              "      <td>Evidence 1</td>\n",
              "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>423A1CA112E2</td>\n",
              "      <td>1.622628e+12</td>\n",
              "      <td>402.0</td>\n",
              "      <td>758.0</td>\n",
              "      <td>When people have phones, they know about certa...</td>\n",
              "      <td>Evidence</td>\n",
              "      <td>Evidence 2</td>\n",
              "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>423A1CA112E2</td>\n",
              "      <td>1.622628e+12</td>\n",
              "      <td>759.0</td>\n",
              "      <td>886.0</td>\n",
              "      <td>Driving is one of the way how to get around. P...</td>\n",
              "      <td>Claim</td>\n",
              "      <td>Claim 1</td>\n",
              "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e0d12c87-bc4d-4897-9767-f548dd50a46a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e0d12c87-bc4d-4897-9767-f548dd50a46a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e0d12c87-bc4d-4897-9767-f548dd50a46a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_dataset['discourse_text'][0] #snippet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SkChPUg1BHuZ",
        "outputId": "c5d04033-b34e-4e17-b544-3f707c2de059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Modern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media. They even do it while driving.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbjfpg4v9oJx"
      },
      "outputs": [],
      "source": [
        "train_df = exp_dataset.sample(frac=0.85)\n",
        "test_df = exp_dataset.copy().drop(train_df.index).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwTEA1sY5AbW"
      },
      "outputs": [],
      "source": [
        "class WritingDataset(data.Dataset):\n",
        "  def __init__(self, data: pd.DataFrame, tokenizer: str):\n",
        "    self.data = data\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
        "    self.label_map = {label: i for i, label in enumerate(data['discourse_type'].unique())}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = self.data.iloc[idx]\n",
        "    encoded = self.tokenizer(sample['discourse_text'], padding='max_length', truncation=True)\n",
        "    label = self.label_map[sample['discourse_type']]\n",
        "    return [\n",
        "        torch.Tensor(encoded['input_ids']).int(), # data\n",
        "        label, # label\n",
        "        torch.Tensor(encoded['attention_mask']).int() # attention mask\n",
        "      ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5zdeM-O5Wsl"
      },
      "outputs": [],
      "source": [
        "# BERT UNCASED\n",
        "bert_train_dataset = WritingDataset(train_df, tokenizer='bert-based-uncased')\n",
        "bert_test_dataset = WritingDataset(test_df, tokenizer='bert-based-uncased')\n",
        "\n",
        "\n",
        "# ROBERTA \n",
        "roberta_train_dataset = WritingDataset(train_df, tokenizer='roberta-large')\n",
        "roberta_test_dataset = WritingDataset(test_df, tokenizer='roberta-large')\n",
        "\n",
        "\n",
        "# DISTILBERT UNCASED\n",
        "distilbert_train_dataset = WritingDataset(train_df, tokenizer='distilbert-base-uncased')\n",
        "distilbert_test_dataset = WritingDataset(test_df, tokenizer='distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVrcaCAM9JGO"
      },
      "outputs": [],
      "source": [
        "# BERT UNCASED\n",
        "bert_train_dataloader = DataLoader(bert_train_dataset, batch_size=16, shuffle=True)\n",
        "bert_test_dataloader = DataLoader(bert_test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# ROBERTA \n",
        "roberta_train_dataloader = DataLoader(roberta_train_dataset, batch_size=16, shuffle=True)\n",
        "roberta_test_dataloader = DataLoader(roberta_test_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# DISTILBERT UNCASED\n",
        "distilbert_train_dataloader = DataLoader(distilbert_train_dataset, batch_size=16, shuffle=True)\n",
        "distilbert_test_dataloader = DataLoader(distilbert_test_dataset, batch_size=16, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "class bertModel(torch.nn.Module):\n",
        " def __init__(\n",
        "     self,\n",
        "     output_dim: int,\n",
        "     hidden_dim: int,\n",
        "     n_layers: int,\n",
        "     bidirectional: bool,\n",
        "     dropout: int,\n",
        "     train_bert: bool = False):\n",
        "  \n",
        "   super().__init__()\n",
        "   self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "   if not train_bert:\n",
        "     for name, param in self.bert.named_parameters():\n",
        "           param.requires_grad = False\n",
        " \n",
        "   embedding_dim = self.bert.config.to_dict()['hidden_size']\n",
        "  \n",
        "   self.rnn = nn.GRU(embedding_dim,\n",
        "                     hidden_dim,\n",
        "                     num_layers = n_layers,\n",
        "                     bidirectional = bidirectional,\n",
        "                     batch_first = True,\n",
        "                     dropout = 0 if n_layers < 2 else dropout)\n",
        "  \n",
        "   self.dropout = nn.Dropout(dropout)\n",
        "   self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        " \n",
        " def forward(self, x):\n",
        " \n",
        "   with torch.no_grad():\n",
        "     embedded = self.bert(x)[0]\n",
        " \n",
        "   _, hidden = self.rnn(embedded)\n",
        " \n",
        "   if self.rnn.bidirectional:\n",
        "       hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "   else:\n",
        "       hidden = self.dropout(hidden[-1,:,:])\n",
        " \n",
        "  \n",
        "   output = self.out(hidden)\n",
        " \n",
        "   return output"
      ],
      "metadata": {
        "id": "R2nzUD4z-bBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5aUW4BxKHHf"
      },
      "outputs": [],
      "source": [
        "#ROBERTA \n",
        "class robertaModel(torch.nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      output_dim: int,\n",
        "      hidden_dim: int,\n",
        "      n_layers: int,\n",
        "      bidirectional: bool,\n",
        "      dropout: int,\n",
        "      train_roberta: bool = False):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.roberta = RobertaModel.from_pretrained('roberta-large')\n",
        "    if not train_roberta:\n",
        "      for name, param in self.roberta.named_parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    embedding_dim = self.roberta.config.to_dict()['hidden_size']\n",
        "    \n",
        "    self.rnn = nn.GRU(embedding_dim,\n",
        "                      hidden_dim,\n",
        "                      num_layers = n_layers,\n",
        "                      bidirectional = bidirectional,\n",
        "                      batch_first = True,\n",
        "                      dropout = 0 if n_layers < 2 else dropout)\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      embedded = self.roberta(x)[0]\n",
        "\n",
        "    _, hidden = self.rnn(embedded)\n",
        "\n",
        "    if self.rnn.bidirectional:\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "    else:\n",
        "        hidden = self.dropout(hidden[-1,:,:])\n",
        "\n",
        "    \n",
        "    output = self.out(hidden)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DISTILBERT\n",
        "class distilbertModel(torch.nn.Module):\n",
        " def __init__(\n",
        "     self,\n",
        "     output_dim: int,\n",
        "     hidden_dim: int,\n",
        "     n_layers: int,\n",
        "     bidirectional: bool,\n",
        "     dropout: int,\n",
        "     train_distilbert: bool = False):\n",
        "  \n",
        "   super().__init__()\n",
        "   self.distilbert = BertModel.from_pretrained('distilbert-base-uncased')\n",
        "   if not train_distilbert:\n",
        "     for name, param in self.distilbert.named_parameters():\n",
        "           param.requires_grad = False\n",
        " \n",
        "   embedding_dim = self.distilbert.config.to_dict()['hidden_size']\n",
        "  \n",
        "   self.rnn = nn.GRU(embedding_dim,\n",
        "                     hidden_dim,\n",
        "                     num_layers = n_layers,\n",
        "                     bidirectional = bidirectional,\n",
        "                     batch_first = True,\n",
        "                     dropout = 0 if n_layers < 2 else dropout)\n",
        "  \n",
        "   self.dropout = nn.Dropout(dropout)\n",
        "   self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        " \n",
        " def forward(self, x):\n",
        " \n",
        "   with torch.no_grad():\n",
        "     embedded = self.distilbert(x)[0]\n",
        " \n",
        "   _, hidden = self.rnn(embedded)\n",
        " \n",
        "   if self.rnn.bidirectional:\n",
        "       hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "   else:\n",
        "       hidden = self.dropout(hidden[-1,:,:])\n",
        " \n",
        "  \n",
        "   output = self.out(hidden)\n",
        " \n",
        "   return output"
      ],
      "metadata": {
        "id": "Ps1iZXQ4Bkzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "#BERT\n",
        "#base\n",
        "bert_optimizer = optim.Adam(bertModel.parameters())\n",
        "\n",
        "#altered\n",
        "bert_tuned_optimizer = optim.Adam(bertModel.parameters(), lr=3e-5, eps=1e-6, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "\n",
        "#ROBERTA\n",
        "#base\n",
        "roberta_optimizer = optim.Adam(robertaModel.parameters())\n",
        "\n",
        "#altered\n",
        "roberta_tuned_optimizer = optim.Adam(robertaModel.parameters(), lr=3e-5, eps=1e-6, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "\n",
        "#DISTILBERT\n",
        "#base\n",
        "distilbert_optimizer = optim.Adam(distilbertModel.parameters())\n",
        "\n",
        "#altered\n",
        "distilbert_tuned_optimizer = optim.Adam(distilbertModel.parameters(), lr=3e-5, eps=1e-6, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "#same for all \n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n"
      ],
      "metadata": {
        "id": "7JA0592CEYGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uG5U3WDBSYfe"
      },
      "outputs": [],
      "source": [
        "#architecture and hyperparams\n",
        "BERT = send_to_device(\n",
        "    robertaModel(\n",
        "      output_dim=len(bert_train_dataset.label_map),\n",
        "      hidden_dim=256,\n",
        "      n_layers=2,\n",
        "      bidirectional=True,\n",
        "      dropout=0,\n",
        "      train_roberta=False\n",
        "  )\n",
        ")\n",
        "\n",
        "ROBERTA = send_to_device(\n",
        "    robertaModel(\n",
        "      output_dim=len(roberta_train_dataset.label_map),\n",
        "      hidden_dim=256,\n",
        "      n_layers=2,\n",
        "      bidirectional=True,\n",
        "      dropout=0,\n",
        "      train_roberta=False\n",
        "  )\n",
        ")\n",
        "\n",
        "DISTILBERT = send_to_device(\n",
        "    robertaModel(\n",
        "      output_dim=len(distilbert_train_dataset.label_map),\n",
        "      hidden_dim=256,\n",
        "      n_layers=2,\n",
        "      bidirectional=True,\n",
        "      dropout=0,\n",
        "      train_roberta=False\n",
        "  )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP-4g2A0J4qF"
      },
      "outputs": [],
      "source": [
        "def accuracy(preds, y):\n",
        "    correct = preds.eq(y.view_as(preds)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mw92I6rl59nn"
      },
      "outputs": [],
      "source": [
        "def train_step(model, optimizer, criterion, train_dataloader):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "  for batch, label, _ in tqdm(train_dataloader):\n",
        "    batch = batch.int().to(device)\n",
        "    label = label.float().to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    preds = torch.argmax(model(batch), axis=1).float()\n",
        "    \n",
        "    acc = accuracy(preds, label)\n",
        "    loss = criterion(preds, label)\n",
        "    loss.requires_grad = True\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "        \n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(train_dataloader), epoch_acc / len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tf2aRbTzKlSo"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, criterion, test_dataloader):\n",
        "  eval_loss = 0\n",
        "  eval_acc = 0\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch, label, _ in test_dataloader:\n",
        "      batch = batch.int().to(device)\n",
        "      label = label.float().to(device)\n",
        "\n",
        "      preds = torch.argmax(model(batch), axis=1).float()\n",
        "      acc = accuracy(preds, label)\n",
        "      loss = criterion(preds, label)\n",
        "\n",
        "      eval_loss += loss.item()\n",
        "      eval_acc += acc.item()\n",
        "\n",
        "  return eval_loss / len(train_dataloader), eval_acc / len(train_dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfsyTo_uk32M"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, train_dataloader, test_dataloader, epochs):\n",
        "  history = {\n",
        "      'train': {\n",
        "          'loss': [],\n",
        "          'acc': []\n",
        "        },\n",
        "      'eval': {\n",
        "          'loss': [],\n",
        "          'acc': []\n",
        "        }\n",
        "      }\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_loss, train_acc = train_step(model, optimizer, criterion, train_dataloader)\n",
        "    eval_loss, eval_acc = evaluate(model, criterion, test_dataloader)\n",
        "    history['train']['loss'].append(train_loss)\n",
        "    history['train']['acc'].append(train_acc)\n",
        "    history['eval']['loss'].append(eval_loss)\n",
        "    history['eval']['acc'].append(eval_acc)\n",
        "    torch.save(model, 'model_{epoch}.pt')\n",
        "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Train Acc: {train_acc*100}, Eval Loss: {eval_loss}, Eval Acc: {eval_acc*100}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpsPYyvjiteB",
        "outputId": "e46eb483-fbe0-4892-df4b-0e22c1bee9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [17:07<00:00,  3.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train Loss: -1.5718411876085085, Train Acc: 6.25, Eval Loss: -0.28306302513939124, Eval Acc: 2.146565495207668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [17:07<00:00,  3.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: -1.558165132904205, Train Acc: 6.3099041533546325, Eval Loss: -0.25892134329762323, Eval Acc: 2.086661341853035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [17:06<00:00,  3.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train Loss: -1.538927958343928, Train Acc: 6.399760383386581, Eval Loss: -0.2698230284471481, Eval Acc: 2.2064696485623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [17:08<00:00,  3.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train Loss: -1.5922786501316597, Train Acc: 6.419728434504793, Eval Loss: -0.2739928555183898, Eval Acc: 2.1765175718849843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 313/313 [17:08<00:00,  3.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Train Loss: -1.5701346629486679, Train Acc: 6.409744408945688, Eval Loss: -0.2634501600036987, Eval Acc: 2.1765175718849843\n"
          ]
        }
      ],
      "source": [
        "# train(model, optimizer, criterion, train_dataloader, test_dataloader, 5)\n",
        "\n",
        "\n",
        "#BERT\n",
        "# train(bertModel, bert_optimizer, criterion, bert_train_dataloader, bert_test_dataloader, 5)\n",
        "\n",
        "# train(bertModel, bert_tuned_optimizer, criterion, bert_train_dataloader, bert_test_dataloader, 5)\n",
        "\n",
        "\n",
        "\n",
        "#ROBERTA\n",
        "train(robertaModel, roberta_optimizer, criterion, roberta_train_dataloader, roberta_test_dataloader, 5)\n",
        "\n",
        "# train(robertaModel, roberta_tuned_optimizer, criterion, roberta_train_dataloader, roberta_test_dataloader, 5)\n",
        "\n",
        "\n",
        "\n",
        "DISTILBERT\n",
        "# train(distilbertModel, distilbert_optimizer, criterion, distilbert_train_dataloader, distilbert_test_dataloader, 5)\n",
        "\n",
        "# train(distilbertModel, distilbert_tuned_optimizer, criterion, distilbert_train_dataloader, distilbert_test_dataloader, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyblQ60ciuT2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBS0wKONj8zp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}