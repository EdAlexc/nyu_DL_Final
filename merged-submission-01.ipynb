{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:20:12.171002Z","iopub.execute_input":"2022-11-28T10:20:12.171839Z","iopub.status.idle":"2022-11-28T10:20:12.196737Z","shell.execute_reply.started":"2022-11-28T10:20:12.171731Z","shell.execute_reply":"2022-11-28T10:20:12.195874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Yev Submission","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport codecs\nfrom text_unidecode import unidecode\nfrom typing import Tuple\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nos.system('pip uninstall -y transformers')\nos.system('pip uninstall -y tokenizers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\nos.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\n\ndef replace_newline(text):\n    text = text.replace('\\n', '[BR]')\n    return text\n\nplaceholders_replacements = {\n    'Generic_School': '[GENERIC_SCHOOL]',\n    'Generic_school': '[GENERIC_SCHOOL]',\n    'SCHOOL_NAME': '[SCHOOL_NAME]',\n    'STUDENT_NAME': '[STUDENT_NAME]',\n    'Generic_Name': '[GENERIC_NAME]',\n    'Genric_Name': '[GENERIC_NAME]',\n    'Generic_City': '[GENERIC_CITY]',\n    'LOCATION_NAME': '[LOCATION_NAME]',\n    'HOTEL_NAME': '[HOTEL_NAME]',\n    'LANGUAGE_NAME': '[LANGUAGE_NAME]',\n    'PROPER_NAME': '[PROPER_NAME]',\n    'OTHER_NAME': '[OTHER_NAME]',\n    'PROEPR_NAME': '[PROPER_NAME]',\n    'RESTAURANT_NAME': '[RESTAURANT_NAME]',\n    'STORE_NAME': '[STORE_NAME]',\n    'TEACHER_NAME': '[TEACHER_NAME]',\n}\ndef replace_placeholders(text):\n    for key, value in placeholders_replacements.items():\n        text = text.replace(key, value)\n    return text\n\n\ndef preprocess_text(text):\n    text = resolve_encodings_and_normalize(text)\n    text = replace_newline(text)\n    text = replace_placeholders(text)\n    return text\n\n\ndef get_max_len_from_df(df, tokenizer, n_special_tokens=3):\n    lengths = []\n    tk0 = tqdm(df['full_text'].fillna(\"\").values, total=len(df))\n    for text in tk0:\n        length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n        lengths.append(length)\n    max_length = max(lengths) + n_special_tokens\n    return max_length\n\n\nclass TestDataset(Dataset):\n    def __init__(self, df):\n        self.texts = df['full_text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        text = self.texts[item]\n        inputs = CFG.tokenizer.encode_plus(\n            text,\n            # corrected_text,\n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=CFG.max_len,\n            pad_to_max_length=True,\n            truncation=True,\n        )\n        for k, v in inputs.items():\n            inputs[k] = torch.tensor(v, dtype=torch.long)\n            \n        return inputs\n    \n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n    \nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n            torch.tensor([1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float)\n        )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor * all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n        return weighted_average[:, 0]\n    \n    \nclass LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm, dropout_rate, is_lstm=True):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n\n        if is_lstm:\n            self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        else:\n            self.lstm = nn.GRU(self.hidden_size, self.hiddendim_lstm, batch_first=True, bidirectional=True)\n\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, all_hidden_states):\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers + 1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        out = self.dropout(out[:, -1, :])\n        return out\n    \nclass ConcatPooling(nn.Module):\n    def __init__(self, n_layers=4):\n        super(ConcatPooling, self, ).__init__()\n\n        self.n_layers = n_layers\n\n    def forward(self, all_hidden_states):\n        concatenate_pooling = torch.cat([all_hidden_states[-(i + 1)] for i in range(self.n_layers)], -1)\n        concatenate_pooling = concatenate_pooling[:, 0]\n        return concatenate_pooling    \n\n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        \n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        else:\n            self.config = torch.load(config_path)\n            \n        if pretrained:\n            self.backbone = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.backbone = AutoModel.from_config(self.config)\n        \n        if cfg.pooling_type == 'MeanPooling':\n            self.pool = MeanPooling()\n        elif cfg.pooling_type == 'WeightedLayerPooling':\n            self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n        elif cfg.pooling_type == 'ConcatPooling':\n            self.pool = ConcatPooling(n_layers=cfg.n_layers)\n            \n        elif cfg.pooling_type == 'LSTMPooling':\n            self.pool =  LSTMPooling(self.config.num_hidden_layers,\n                                       self.config.hidden_size,\n                                       self.cfg.hidden_size,\n                                       0.1,\n                                       is_lstm=True\n                           )\n        else:\n            raise ValueError('Unknown pooling type')\n        \n        \n        if cfg.pooling_type == 'GRUPooling':\n            self.fc = nn.Linear(self.cfg.hidden_size, 6)\n        elif cfg.pooling_type == 'LSTMPooling':\n            self.fc = nn.Linear(self.cfg.hidden_size, 6)\n        elif cfg.pooling_type == 'ConcatPooling':\n            self.fc = nn.Linear(cfg.n_layers*self.config.hidden_size, 6)\n        else:\n            self.fc = nn.Linear(self.config.hidden_size, 6)\n\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.backbone(**inputs)\n        \n        last_hidden_states = outputs[0]\n        \n        if self.cfg.pooling_type == 'MeanPooling':\n            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        elif self.cfg.pooling_type == 'WeightedLayerPooling':\n            all_hidden_states = torch.stack(outputs[1])\n            feature = self.pool(all_hidden_states)\n        elif self.cfg.pooling_type == 'ConcatPooling':\n            all_hidden_states = torch.stack(outputs[1])\n            feature = self.pool(all_hidden_states)\n        elif self.cfg.pooling_type in ['GRUPooling', 'LSTMPooling']:\n            all_hidden_states = torch.stack(outputs[1])\n            feature = self.pool(all_hidden_states)\n        else:\n            raise ValueError('Unknown pooling type')\n        \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output\n    \n    \ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        inputs = collate(inputs)\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\n\n\nclass CFG_model23:\n    num_workers=4\n    path=\"../input/model23/\"\n    model_name = 'model23'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=12\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'MeanPooling'\n\n    \n    \nclass CFG_model52:\n    num_workers=4\n    path=\"../input/model52/\"\n    model_name = 'model52'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=12\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'MeanPooling'\n    \n    \nclass CFG_model68:\n    num_workers=4\n    path=\"../input/model68/\"\n    model_name = 'model68'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=8\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'WeightedLayerPooling'\n    \n    \nclass CFG_model70:\n    num_workers=4\n    path=\"../input/model70/\"\n    model_name = 'model70'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=8\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'LSTMPooling'\n    hidden_size = 512\n    \n    \nclass CFG_model71:\n    num_workers=4\n    path=\"../input/model71/\"\n    model_name = 'model71'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=8\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'WeightedLayerPooling'\n    \n    \nclass CFG_model75:\n    num_workers=4\n    path=\"../input/model75/\"\n    model_name = 'model75'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=8\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'ConcatPooling'\n    n_layers = 4    \n\nclass CFG_model55:\n    num_workers=4\n    path=\"../input/model55/\"\n    model_name = 'model55'\n    config_path=path+'config.pth'\n    model=\"microsoft-deberta-v3-large\"\n    gradient_checkpointing=False\n    batch_size=12\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5 \n    trn_fold=[0, 1, 2, 3, 4]\n    max_len=1462\n    set_from_df = True\n    pooling_type = 'MeanPooling'\n    \n    \ncfg_list = [\n    CFG_model23,\n    CFG_model52, \n    CFG_model55,\n    CFG_model68,\n    CFG_model70,\n    CFG_model71,\n    CFG_model75\n]\n\n\n# cfg_list = [CFG_model23, CFG_model52, CFG_model55]\n# cfg_list = [CFG_model23, CFG_model52]\n\n\nfor CFG in cfg_list:\n    print(CFG.model_name)\n    \n    CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.path+'tokenizer/')\n    \n    test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n    test['full_text'] = test['full_text'].apply(preprocess_text)\n    submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n    \n    # sort by length to speed up inference\n    test['tokenize_length'] = [len(CFG.tokenizer(text)['input_ids']) for text in test['full_text'].values]\n    test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n    \n    if CFG.set_from_df:\n        CFG.max_len = get_max_len_from_df(test, CFG.tokenizer)\n    print(CFG.max_len)\n    test_dataset = TestDataset(test)\n\n    test_loader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size,\n                             shuffle=False,\n                             collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n                             num_workers=CFG.num_workers, \n                             pin_memory=True, \n                             drop_last=False)\n\n    predictions = []\n    for fold in CFG.trn_fold:\n        \n        if test.shape[0] == 3 and fold > 0:\n            continue\n        \n        model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n        state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu'))\n        \n        model.load_state_dict(state['model'])\n        prediction = inference_fn(test_loader, model, device)\n        predictions.append(prediction)\n        \n        del model, state, prediction; gc.collect()\n        torch.cuda.empty_cache()\n        \n    predictions = np.mean(predictions, axis=0)\n\n    test[CFG.target_cols] = predictions\n    submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n    display(submission.head())\n    submission[['text_id'] + CFG.target_cols].to_csv(f'submission_{CFG.model_name}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:20:12.328009Z","iopub.execute_input":"2022-11-28T10:20:12.328835Z","iopub.status.idle":"2022-11-28T10:20:53.534346Z","shell.execute_reply.started":"2022-11-28T10:20:12.328792Z","shell.execute_reply":"2022-11-28T10:20:53.532675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### nischay","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom tqdm.auto import tqdm\n\nimport numpy as np \nimport pandas as pd \nfrom transformers import get_polynomial_decay_schedule_with_warmup,get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\nfrom transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\nfrom transformers import DataCollatorWithPadding,DataCollatorForTokenClassification\n\n\nfrom text_unidecode import unidecode\nfrom typing import Dict, List, Tuple\nimport codecs\nfrom datasets import concatenate_datasets,load_dataset,load_from_disk\n\nfrom sklearn.metrics import log_loss\n\nfrom transformers import AutoModel, AutoTokenizer, AdamW, DataCollatorWithPadding\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom datasets import Dataset, load_from_disk\nimport pickle\nimport re\nfrom transformers import TrainingArguments, AutoConfig, AutoModelForTokenClassification, DataCollatorForTokenClassification\n\n\nclass config:\n    base_dir = \"/kaggle/working/\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    seed = 69\n    # dataset path \n    train_dataset_path = \"../input/feedback-prize-english-language-learning/train.csv\"\n    test_dataset_path = \"../input/feedback-prize-english-language-learning/test.csv\"\n    sample_submission_path = \"../input/feedback-prize-english-language-learning/sample_submission.csv\"\n       \n    save_dir=\"../input/colab-models-download-v2-0/\"\n    \n    #tokenizer params\n    truncation = True \n    padding = False #'max_length'\n    max_length = 512\n    \n    # model params\n    model_name = \"microsoft/deberta-v3-large\"\n    target_cols = ['cohesion', 'syntax', 'vocabulary',\n       'phraseology', 'grammar', 'conventions']\n    load_from_disk = None\n    \n    #training params\n    learning_rate = 9e-6\n    batch_size = 2\n    epochs = 3\n    NFOLDS = 5\n\nseed_everything(config.seed)\n\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\ndf_train = pd.read_csv(config.train_dataset_path)\ndf_test = pd.read_csv(config.test_dataset_path)\ndf_ss = pd.read_csv(config.sample_submission_path)\n\ndf_train['full_text'] = df_train['full_text'].apply(resolve_encodings_and_normalize)\ndf_test['full_text'] = df_test['full_text'].apply(resolve_encodings_and_normalize)\ndf_test[config.target_cols] = 0.\n\ntokenizer = AutoTokenizer.from_pretrained(\"../input/deberta-v3-large/deberta-v3-large\")\n\ndef tokenize(example):\n    text = example[\"full_text\"]\n\n    tokenized = tokenizer(text,\n        padding=config.padding,\n        truncation=True,\n        max_length=config.max_length,\n        add_special_tokens=True,\n    )\n\n#     print(tokenized)\n\n    tokenized[\"labels\"] = [example[i] for i in config.target_cols]\n    tokenized[\"length\"] = len(tokenized[\"input_ids\"])\n    \n    return tokenized\n\n\nif config.load_from_disk is None:\n\n  ds = Dataset.from_pandas(df_test)\n\n  ds = ds.map(\n      tokenize,\n      batched=False,\n      num_proc=4,\n      desc=\"Tokenizing\",\n  )\n\n  ds.save_to_disk(f\"{config.base_dir}data.dataset\")\n  with open(f\"{config.base_dir}_pkl\", \"wb\") as fp:\n      pickle.dump(df_test, fp)\n  print(\"Saving dataset to disk:\", config.save_dir)\n\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n    \nclass FeedbackPrizeModel(pl.LightningModule):\n    def __init__(self,config):\n        super().__init__()\n        self.cfg = config\n\n        self.model_config = AutoConfig.from_pretrained('../input/deberta-v3-large/deberta-v3-large/config.json')\n        self.model_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": 0.,\n                \"add_pooling_layer\": False,\n                \"attention_probs_dropout_prob\":0,\n            }\n        )\n        \n        self.transformers_model = AutoModel.from_pretrained(\"../input/deberta-v3-large/deberta-v3-large\",config=self.model_config)\n\n        # self.classifier =  nn.Linear(self.transformers_model.config.hidden_size,len(self.cfg.target_cols))\n\n        self.layer_norm = nn.LayerNorm(self.transformers_model.config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.transformers_model.config.hidden_size * 2, len(self.cfg.target_cols))   #  + num_external_features\n\n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n\n        self.dropouts = nn.ModuleList([\n                nn.Dropout(0.1*i) for i in range(5)\n            ])\n        self.loss_function = nn.SmoothL1Loss(reduction='mean') \n        \n    def forward(self, input_ids, attention_mask,train):\n        \n        output_backbone = self.transformers_model(input_ids,attention_mask = attention_mask)#[0]\n       # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)[:,0,:]\n\n        # print(output_backbone.shape)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.output(output_backbone)\n            else:\n                logits += self.output(output_backbone)\n        \n        logits /= len(self.dropouts)\n        return (logits, _)\n    \n        \n    def train_dataloader(self):\n        return self._train_dataloader \n    \n    def validation_dataloader(self):\n        return self._validation_dataloader\n\n    def get_optimizer_params(self, encoder_lr, decoder_lr, weight_decay=0.0):\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {'params': [p for n, p in self.transformers_model.named_parameters() if not any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': weight_decay},\n            {'params': [p for n, p in self.transformers_model.named_parameters() if any(nd in n for nd in no_decay)],\n             'lr': encoder_lr, 'weight_decay': 0.0},\n            {'params': [p for n, p in self.named_parameters() if \"transformers_model\" not in n],\n             'lr': decoder_lr, 'weight_decay': 0.0}\n        ]\n        return optimizer_parameters\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr = config.learning_rate)\n\n        epoch_steps = self.cfg.data_length\n        batch_size = self.cfg.batch_size\n\n        warmup_steps = 0.1 * epoch_steps // batch_size\n        training_steps = self.cfg.epochs * epoch_steps // batch_size\n        # scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,training_steps,-1)\n        scheduler = get_polynomial_decay_schedule_with_warmup(optimizer, warmup_steps, training_steps, lr_end=7e-7, power=3.0)\n\n        lr_scheduler_config = {\n                'scheduler': scheduler,\n                'interval': 'step',\n                'frequency': 1,\n            }\n\n        return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler_config}\n    \n    \ndef predict(data_loader, model):\n        \n    model.to(config.device)\n    model.eval()    \n    predictions = []\n    for batch in tqdm(data_loader):\n\n        with torch.no_grad():\n            inputs = {key:val.reshape(val.shape[0], -1).to(config.device) for key,val in batch.items()}\n            outputs = model(input_ids = inputs['input_ids'], attention_mask = inputs['attention_mask'],train=False)[0]\n        predictions.extend(outputs.detach().cpu().numpy())\n    predictions = np.vstack(predictions)\n    return predictions\n\n\n\ncollator = DataCollatorWithPadding(\n    tokenizer=tokenizer, pad_to_multiple_of= 16, padding=\"longest\"\n)\n\nfor fold in range(config.NFOLDS):\n    \n    if test.shape[0] == 3 and fold > 0:\n        continue\n    \n    train_ds_list = []\n    print(f\"====== FOLD RUNNING {fold}======\")\n    \n    keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n\n    test_ds = load_from_disk(f'{config.base_dir}data.dataset').sort(\"length\")\n    test_ds = test_ds.remove_columns([c for c in test_ds.column_names if c not in keep_cols])\n    config.data_length = len(test_ds)\n\n    print('Dataset Loaded....')\n    print((test_ds[0].keys()))\n\n    print(\"Generating Test DataLoader\")\n    test_dataloader = DataLoader(test_ds, batch_size = config.batch_size, shuffle = False, num_workers= 2, pin_memory=True,collate_fn = collator)\n\n    \n    print(\"Model Creation\")\n\n    model = FeedbackPrizeModel.load_from_checkpoint(f'{config.save_dir}microsoft/deberta-v3-large_{fold}.ckpt',train_dataloader=None,validation_dataloader=None,config=config)    \n    preds = predict(test_dataloader, model)   \n\n    if fold==0:\n        final_preds = preds * (1/config.NFOLDS)\n    else:\n        final_preds += preds * (1/config.NFOLDS)\n\n    del model,test_dataloader,test_ds\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    \ntest_ids = []\ntest_ds = load_from_disk(f'{config.base_dir}data.dataset').sort(\"length\")\nfor i in test_ds:\n    test_ids.append(i['text_id'])\n\nprint(final_preds.shape)\n\nsub_df = pd.DataFrame(test_ids,columns={\"text_id\"})\nsub_df[config.target_cols] = final_preds\nsub_df.to_csv('submission_nischay.csv',index=False)\nprint(sub_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:05:43.127642Z","iopub.execute_input":"2022-11-25T21:05:43.128424Z","iopub.status.idle":"2022-11-25T21:06:27.519353Z","shell.execute_reply.started":"2022-11-25T21:05:43.128375Z","shell.execute_reply":"2022-11-25T21:06:27.518001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KOJIMAR","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=false\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\n    \nclass CFG1:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0926-deberta-v3-large-unscale/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    model_name = '0926-deberta-v3-large'\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base+'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=6\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG2:\n    model = \"microsoft/deberta-v2-xlarge-mnli\"\n    path = \"../input/0919-deberta-v2-xlarge-mnli/\"\n    base = \"../input/fb3models/microsoft-deberta-v2-xlarge/\"\n    config_path = base + \"config/config.json\"\n    model_name = '0919-deberta-v2-xlarge-mnli'\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=4\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=5\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \nclass CFG3:\n    model = \"microsoft/deberta-v3-large\"\n    path = \"../input/0911-deberta-v3-large/\"\n    base = \"../input/fb3models/microsoft-deberta-v3-large/\"\n    model_name = '0911-deberta-v3-large'\n    config_path = base + \"config/config.json\"\n    tokenizer = AutoTokenizer.from_pretrained(base + 'tokenizer/')\n    gradient_checkpointing=False\n    batch_size=6\n    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n    seed=42\n    n_fold=10\n    trn_fold=list(range(n_fold))\n    num_workers=4\n    weight = 1.0\n    \n\n    \nCFG_list = [CFG1, CFG2, CFG3]\n\n# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n\n# ====================================================\n# oof\n# ====================================================\nfor CFG in CFG_list:\n    oof_df = pd.read_pickle(CFG.path+'oof_df.pkl')\n    labels = oof_df[CFG.target_cols].values\n    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n    score, scores = get_score(labels, preds)\n    LOGGER.info(f'Model: {CFG.model} Score: {score:<.4f}  Scores: {scores}')\n    \n    \n# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text):\n    inputs = cfg.tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        #max_length=CFG.max_len,\n        #pad_to_max_length=True,\n        #truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.texts = df['full_text'].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, self.texts[item])\n        return inputs\n    \n    \n# ====================================================\n# Model\n# ====================================================\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\nclass MaxPooling(nn.Module):\n    def __init__(self):\n        super(MaxPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = -1e4\n        max_embeddings, _ = torch.max(embeddings, dim = 1)\n        return max_embeddings\n    \nclass MinPooling(nn.Module):\n    def __init__(self):\n        super(MinPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        embeddings = last_hidden_state.clone()\n        embeddings[input_mask_expanded == 0] = 1e-4\n        min_embeddings, _ = torch.min(embeddings, dim = 1)\n        return min_embeddings\n        \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        else:\n            self.config = AutoConfig.from_pretrained(config_path, output_hidden_states=True)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        if self.cfg.gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n        self.pool = MeanPooling()\n        self.fc = nn.Linear(self.config.hidden_size, 6)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output\n    \n    \n# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\n\nfor _idx, CFG in enumerate(CFG_list):\n    test = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n    submission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n    # sort by length to speed up inference\n    test['tokenize_length'] = [len(CFG.tokenizer(text)['input_ids']) for text in test['full_text'].values]\n    test = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\n\n    test_dataset = TestDataset(CFG, test)\n    test_loader = DataLoader(test_dataset,\n                             batch_size=CFG.batch_size,\n                             shuffle=False,\n                             collate_fn=DataCollatorWithPadding(tokenizer=CFG.tokenizer, padding='longest'),\n                             num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n    predictions = []\n    for fold in CFG.trn_fold:\n        if test.shape[0] == 3 and fold > 0:\n            continue\n        \n        model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n        state = torch.load(CFG.path+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\",\n                           map_location=torch.device('cpu'))\n        model.load_state_dict(state['model'])\n        prediction = inference_fn(test_loader, model, device)\n        predictions.append(prediction)\n        \n        del model, state, prediction; gc.collect()\n        torch.cuda.empty_cache()\n        \n    predictions = np.mean(predictions, axis=0)\n    test[CFG.target_cols] = predictions\n    submission = submission.drop(columns=CFG.target_cols).merge(test[['text_id'] + CFG.target_cols], on='text_id', how='left')\n    display(submission.head())\n    submission[['text_id'] + CFG.target_cols].to_csv(f'submission_{CFG.model_name}.csv', index=False)\n    \n    del test, submission, predictions, test_dataset, test_loader; gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:06:27.525067Z","iopub.execute_input":"2022-11-25T21:06:27.527756Z","iopub.status.idle":"2022-11-25T21:08:29.079884Z","shell.execute_reply.started":"2022-11-25T21:06:27.527705Z","shell.execute_reply":"2022-11-25T21:08:29.07873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rohit","metadata":{}},{"cell_type":"code","source":"import codecs\nfrom text_unidecode import unidecode\nfrom typing import Tuple\n\ndef replace_encoding_with_utf8(error: UnicodeError) -> Tuple[bytes, int]:\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error: UnicodeError) -> Tuple[str, int]:\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\n\ndef resolve_encodings_and_normalize(text: str) -> str:\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\n\ndef replace_newline(text):\n    text = text.replace('\\n', '[BR]')\n    return text\n\nplaceholders_replacements = {\n    'Generic_School': '[GENERIC_SCHOOL]',\n    'Generic_school': '[GENERIC_SCHOOL]',\n    'SCHOOL_NAME': '[SCHOOL_NAME]',\n    'STUDENT_NAME': '[STUDENT_NAME]',\n    'Generic_Name': '[GENERIC_NAME]',\n    'Genric_Name': '[GENERIC_NAME]',\n    'Generic_City': '[GENERIC_CITY]',\n    'LOCATION_NAME': '[LOCATION_NAME]',\n    'HOTEL_NAME': '[HOTEL_NAME]',\n    'LANGUAGE_NAME': '[LANGUAGE_NAME]',\n    'PROPER_NAME': '[PROPER_NAME]',\n    'OTHER_NAME': '[OTHER_NAME]',\n    'PROEPR_NAME': '[PROPER_NAME]',\n    'RESTAURANT_NAME': '[RESTAURANT_NAME]',\n    'STORE_NAME': '[STORE_NAME]',\n    'TEACHER_NAME': '[TEACHER_NAME]',\n}\ndef replace_placeholders(text):\n    for key, value in placeholders_replacements.items():\n        text = text.replace(key, value)\n    return text\n\n\ndef pad_punctuation(text):\n    text = re.sub('([.,!?()-])', r' \\1 ', text)\n    text = re.sub('\\s{2,}', ' ', text)\n    return text\n\n\ndef preprocess_text(text):\n    text = resolve_encodings_and_normalize(text)\n    # text = replace_newline(text) \n    text = replace_placeholders(text)\n#     text = pad_punctuation(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:08:29.081391Z","iopub.execute_input":"2022-11-25T21:08:29.081784Z","iopub.status.idle":"2022-11-25T21:08:29.093523Z","shell.execute_reply.started":"2022-11-25T21:08:29.081727Z","shell.execute_reply":"2022-11-25T21:08:29.091834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Library\n# ====================================================\n\nimport os\nimport gc\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom transformers import DataCollatorWithPadding\n%env TOKENIZERS_PARALLELISM=false\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nmodels = [\n    {\n        \"model_name\": \"microsoft/deberta-v3-base\",\n        \"model_path\": \"../input/exp01-fb3-part2/microsoft-deberta-v3-base-pl\",\n        \"tok_path\": \"../input/exp01-fb3-part2/microsoft-deberta-v3-base-pl/tokenizer/\",\n        \"pool\": \"MeanPool\",\n        \"batch_size\": 12,\n        'id': 'exp01_fb3_part2',\n        'preprocess': False\n    },\n    \n    {\n        \"model_name\": \"roberta-large\",\n        \"model_path\": \"../input/exp14-fb3\",\n        \"tok_path\": \"../input/exp14-fb3/tokenizer/\",\n        \"pool\": \"GeM\",\n        \"batch_size\": 6,\n        'id': 'exp14_fb3_roberta',\n        'preprocess': False\n    },\n    \n    {\n        \"model_name\": \"microsoft/deberta-v3-large\",\n        \"model_path\": \"../input/exp13-fb3\",\n        \"tok_path\": \"../input/exp13-fb3/tokenizer/\",\n        \"pool\": \"WLP\",\n        \"batch_size\": 6,\n        'id': 'exp13_fb3',\n        'preprocess': False\n    },\n    {\n        \"model_name\": \"distilbert-base-uncased\",\n        \"model_path\": \"../input/exp02-fb3-part2/distilbert-base-uncased\",\n        \"tok_path\": \"../input/exp02-fb3-part2/distilbert-base-uncased/tokenizer/\",\n        \"pool\": \"ConcatPool\",\n        \"batch_size\": 12,\n        'id': 'exp02_fb3_part2_distilbert',\n        'preprocess': False\n\n    },\n#     {\n#         \"model_name\": \"microsoft/deberta-v3-base\",\n#         \"model_path\": \"../input/exp01-fb3-trainpl/microsoft-deberta-v3-base\",\n#         \"tok_path\": \"../input/exp01-fb3-trainpl/microsoft-deberta-v3-base/tokenizer/\",\n#         \"pool\": \"MeanPool\",\n#         \"batch_size\": 12,\n#         'id': 'exp01_fb3_pl_train',\n#         'preprocess': True\n#     },\n\n    \n#     {\n#         \"model_name\": \"roberta-large\",\n#         \"model_path\": \"../input/exp14-fb3-trainpl/roberta-large\",\n#         \"tok_path\": \"../input/exp14-fb3-trainpl/roberta-large/tokenizer/\",\n#         \"pool\": \"GeM\",\n#         \"batch_size\": 6,\n#         'id': 'exp14_fb3_pl_train',\n#         'preprocess': True\n#     },\n    \n    {\n        \"model_name\": \"microsoft/deberta-v3-base\",\n        \"model_path\": \"../input/exp11-fb3-01/microsoft-deberta-v3-base\",\n        \"tok_path\": \"../input/exp11-fb3-01/microsoft-deberta-v3-base/tokenizer/\",\n        \"pool\": \"MeanPool\",\n        \"batch_size\": 12,\n        'id': 'exp11_fb3_pl_train',\n        'preprocess': True\n    },\n\n    {\n        \"model_name\": \"roberta-large\",\n        \"model_path\": \"../input/exp11-fb3-02/roberta-large\",\n        \"tok_path\": \"../input/exp11-fb3-02/roberta-large/tokenizer/\",\n        \"pool\": \"MeanPool\",\n        \"batch_size\": 6,\n        'id': 'exp11_fb3_rlarge',\n        'preprocess': True\n    },\n    {\n        \"model_name\": \"microsoft/deberta-large\",\n        \"model_path\": \"../input/exp11-fb3-03/microsoft-deberta-large\",\n        \"tok_path\": \"../input/exp11-fb3-03/microsoft-deberta-large/tokenizer/\",\n        \"pool\": \"MeanPool\",\n        \"batch_size\": 6,\n        'id': 'exp11_fb3_dlarge',\n        'preprocess': True\n    },\n    \n# tmp \n#     {\n#         \"model_name\": \"microsoft/deberta-v3-large\",\n#         \"model_path\": \"../input/exp02-fb3\",\n#         \"tok_path\": \"../input/exp02-fb3/tokenizer/\",\n#         \"pool\": \"ConcatPool\",\n#         \"batch_size\": 12,\n#         'id': 'exp02_fb3',\n#         'preprocess': False\n#     },\n    \n#     {\n#         \"model_name\": \"roberta-large\",\n#         \"model_path\": \"../input/exp12-fb3\",\n#         \"tok_path\": \"../input/exp12-fb3/tokenizer/\",\n#         \"pool\": \"ConcatPool\",\n#         \"batch_size\": 6,\n#         'id': 'exp12_fb3',\n#         'preprocess': False\n#     },\n    \n\n\n    \n]\n\n\nnum_workers=4\ngradient_checkpointing=False\ntarget_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\nseed=42\nn_fold=5\ntrn_fold=[0, 1, 2, 3, 4]\n\ntokenizer = AutoTokenizer.from_pretrained(f\"{models[0]['tok_path']}\")\n\n# ====================================================\n# Utils\n# ====================================================\ndef MCRMSE(y_trues, y_preds):\n    scores = []\n    idxes = y_trues.shape[1]\n    for i in range(idxes):\n        y_true = y_trues[:,i]\n        y_pred = y_preds[:,i]\n        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n        scores.append(score)\n    mcrmse_score = np.mean(scores)\n    return mcrmse_score, scores\n\n\ndef get_score(y_trues, y_preds):\n    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n    return mcrmse_score, scores\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n# ====================================================\n# Data Loading \n# ====================================================\ntest = pd.read_csv('../input/feedback-prize-english-language-learning/test.csv')\n\nsubmission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"submission.shape: {submission.shape}\")\ndisplay(submission.head())\n\n# sort by length to speed up inference\ntest['tokenize_length'] = [len(tokenizer(text)['input_ids']) for text in test['full_text'].values]\ntest = test.sort_values('tokenize_length', ascending=True).reset_index(drop=True)\ndisplay(test.head())\n\n\n# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(text,tokenizer, model_type):\n    \n    if \"roberta\" in model_type or \"distilbert\" in model_type :\n        inputs = tokenizer.encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=512, \n            pad_to_max_length=True, \n            truncation=True \n        )\n    else:\n        inputs = tokenizer.encode_plus(\n            text, \n            return_tensors=None, \n            add_special_tokens=True, \n            max_length=768, \n            pad_to_max_length=True, \n            truncation=True \n        )\n        \n    for k, v in inputs.items(): \n        inputs[k] = torch.tensor(v, dtype=torch.long) \n    return inputs \n\n\nclass TestDataset(Dataset):\n    def __init__(self, df,tokenizer, model_type=None):\n        self.texts = df['full_text'].values\n        self.model_type=model_type\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.texts[item],self.tokenizer, self.model_type)\n        return inputs\n    \n    \n# MeanPool\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n# WLP \nclass WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, features):\n        ft_all_layers = features['all_layer_embeddings']\n\n        all_layer_embedding = torch.stack(ft_all_layers)\n        all_layer_embedding = all_layer_embedding[self.layer_start:, :, :, :]\n\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum()\n\n        features.update({'token_embeddings': weighted_average})\n        return features\n\n# GeM\nclass GeMText(nn.Module):\n    def __init__(self, dim=1, cfg=None, p=3, eps=1e-6):\n        super(GeMText, self).__init__()\n        self.dim = dim\n        self.p = Parameter(torch.ones(1) * p)\n        self.eps = eps\n        self.feat_mult = 1\n        # x seeems last hidden state\n\n    def forward(self, x, attention_mask):\n        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(x.shape)\n        x = (x.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n        ret = ret.pow(1 / self.p)\n        return ret\n    \n\n# ====================================================\n# Model\n# ====================================================\n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.config = torch.load(config_path)\n        \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg[\"model_name\"], config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n            \n        if gradient_checkpointing:\n            self.model.gradient_checkpointing_enable()\n           \n        if cfg[\"pool\"] == \"MeanPool\" or cfg[\"pool\"] == \"ConcatPool\":\n            self.pooling = MeanPooling()\n        elif cfg[\"pool\"] == \"WLP\":\n            self.pooling = WeightedLayerPooling(self.config.num_hidden_layers, layer_start=9)\n        elif cfg[\"pool\"] == \"GeM\":\n            self.pooling = GeMText()\n            \n        if cfg[\"pool\"] == \"ConcatPool\":\n            self.head = nn.Linear(self.config.hidden_size*4, 6)       \n        else:\n            self.head = nn.Linear(self.config.hidden_size, 6)\n            \n        if 'facebook/bart' in cfg[\"model_name\"] or 'distilbart' in cfg[\"model_name\"]:\n            self.config.use_cache = False\n            self.initializer_range = self.config.init_std\n        else:\n            self.initializer_range = self.config.initializer_range\n        \n            \n            \n        self._init_weights(self.head)\n        \n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        attention_mask = inputs[\"attention_mask\"]\n        input_ids = inputs[\"input_ids\"]\n        \n        if self.cfg[\"pool\"] == \"WLP\":\n            x = self.model(input_ids=input_ids, attention_mask=attention_mask)\n            tmp = {\n                'all_layer_embeddings': x.hidden_states\n            }\n            feature = self.pooling(tmp)['token_embeddings'][:, 0]\n        \n        elif self.cfg[\"pool\"] == \"ConcatPool\":\n            \n            \n            if 'facebook/bart' in self.cfg[\"model_name\"] or 'distilbart' in self.cfg[\"model_name\"]:\n                x = torch.stack(self.model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True).decoder_hidden_states)\n            else:\n                x = torch.stack(self.model(input_ids=input_ids, attention_mask=attention_mask).hidden_states)\n            \n            p1 = self.pooling(x[-1], attention_mask)\n            p2 = self.pooling(x[-2], attention_mask)\n            p3 = self.pooling(x[-3], attention_mask)\n            p4 = self.pooling(x[-4], attention_mask)\n\n            feature = torch.cat(\n                (p1, p2, p3, p4),-1\n            )\n        else:\n            outputs = self.model(**inputs)\n            x = outputs[0]\n            feature = self.pooling(x, inputs['attention_mask'])\n            \n        return feature\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.head(feature)\n        return output\n    \n    \n# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\n\nall_preds = []\n\nfor cfg in models:\n    \n    tokenizer = AutoTokenizer.from_pretrained(f\"{cfg['tok_path']}\")\n    \n    if cfg['preprocess']:\n        test['full_text'] = test['full_text'].apply(preprocess_text)\n\n    \n    test_dataset = TestDataset(test, tokenizer, cfg[\"model_name\"])\n    batch_size=cfg[\"batch_size\"]\n\n    test_loader = DataLoader(test_dataset,\n                         batch_size=batch_size,\n                         shuffle=False,\n                         collate_fn=DataCollatorWithPadding(tokenizer=tokenizer, padding='longest'),\n                         num_workers=num_workers, pin_memory=True, drop_last=False)\n    \n    \n    predictions = []\n    for fold in trn_fold:\n        \n        if test.shape[0] == 3 and fold > 0:\n            continue\n        \n        model = CustomModel(cfg, config_path=cfg[\"model_path\"]+\"/config.pth\", pretrained=False)    \n        state = torch.load(f\"{cfg['model_path']}/checkpoint_{fold}.pth\",\n                       map_location=torch.device('cpu'))\n\n        model.load_state_dict(state['model'])\n        prediction = inference_fn(test_loader, model, device)\n        predictions.append(prediction)\n        \n        del model, state, prediction; gc.collect()\n        torch.cuda.empty_cache()\n        \n    predictions = np.mean(predictions, axis=0)\n    \n    test_sub = test.copy()\n    test_sub[target_cols] = predictions\n    \n    test_sub.to_csv(f'submission_{cfg[\"id\"]}.csv', index=False)\n    del tokenizer, test_dataset, test_loader; gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:08:29.097434Z","iopub.execute_input":"2022-11-25T21:08:29.098404Z","iopub.status.idle":"2022-11-25T21:11:06.660028Z","shell.execute_reply.started":"2022-11-25T21:08:29.098364Z","shell.execute_reply":"2022-11-25T21:11:06.658994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fb_Bart_Large Sub","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, gc, re, warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import AutoModel,AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append('../input/iterative-stratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ntrain_df = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/test.csv\")\n\ntrain_df[\"src\"]=\"train\"\ntest_df[\"src\"]=\"test\"\n\ndf = pd.concat([train_df, test_df], ignore_index=True)\n\ntarget_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]\n\nFOLDS = 25\n\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\nfor i,(train_index, val_index) in enumerate(skf.split(train_df, train_df[target_cols])):\n    train_df.loc[val_index,'fold'] = i\n    \ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nclass EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_length, tokenizer):\n        self.df = df.reset_index(drop=True)\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        text = self.df.loc[idx,\"full_text\"]\n        tokens = self.tokenizer(\n                        text,\n                        None,\n                        add_special_tokens=True,\n                        padding='max_length',\n                        truncation=True,\n                        max_length=self.max_length,\n                        return_tensors=\"pt\")\n        \n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens\n    \n    \ndef get_embeddings(df, model_name, max_length, batch_size, device, verbose=True):\n    model = AutoModel.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    dataset = EmbedDataset(df, max_length, tokenizer)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n    \n    model = model.to(device)\n    model.eval()\n    all_train_text_feats = []\n    \n    for batch in tqdm(dataloader, total=len(dataloader)):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        \n        with torch.no_grad():\n            model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        all_train_text_feats.extend(sentence_embeddings)\n        \n    all_train_text_feats = np.array(all_train_text_feats)\n    if verbose:\n        print('Embeddings shape',all_train_text_feats.shape)\n        \n    return all_train_text_feats\n\n\nmodel_name = '../input/d/kozodoi/transformers/facebook-bart-large'\n\nwith open(os.path.join('../input/fb3-embeddings/fb_bart_large.npy'), 'rb') as file:\n    fb_bart_large_embeddings = np.load(file)\n    \nfb_bart_large_embeddings_test = get_embeddings(test_df, model_name, max_length=512, batch_size=4, device='cuda',)\n\n\n\nall_train_text_feats = np.concatenate([\n    fb_bart_large_embeddings,\n],axis=1)\n\n\nte_text_feats = np.concatenate([\n    fb_bart_large_embeddings_test,\n],axis=1)\n\n\ngc.collect()\nprint('Our concatenated embeddings have shape', all_train_text_feats.shape )\n\nfrom cuml.svm import SVR\nimport cuml\nprint('RAPIDS version',cuml.__version__)\n\nfrom sklearn.metrics import mean_squared_error\n\npreds = []\nscores = []\ndef comp_score(y_true,y_pred):\n    rmse_scores = []\n    for i in range(len(target_cols)):\n        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n    return np.mean(rmse_scores)\n\nfor fold in tqdm(range(FOLDS),total=FOLDS):\n# for fold in range(FOLDS):\n#     print('# =========================== Fold',fold+1, ' ===========================')\n    \n    dftr_ = train_df[train_df[\"fold\"]!=fold]\n    dfev_ = train_df[train_df[\"fold\"]==fold]\n    \n    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n    \n    ev_preds = np.zeros((len(ev_text_feats),6))\n    test_preds = np.zeros((len(te_text_feats),6))\n    for i,t in enumerate(target_cols):\n        clf = SVR(C=1)\n        clf.fit(tr_text_feats, dftr_[t].values)\n        ev_preds[:,i] = clf.predict(ev_text_feats)\n        test_preds[:,i] = clf.predict(te_text_feats)\n        \n    score = comp_score(dfev_[target_cols].values,ev_preds)\n    scores.append(score)\n#     print(\"Fold : {} RSME score: {}\".format(fold,score))\n    preds.append(test_preds)\n    \n\nprint('\\nOverall CV RSME =',np.mean(scores))\n\nsub = test_df.copy()\n\nsub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\nsub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\nsub = sub[sub_columns]\nsub.to_csv(\"submission_fb_bart_large.csv\",index=None)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:11:06.661949Z","iopub.execute_input":"2022-11-25T21:11:06.66225Z","iopub.status.idle":"2022-11-25T21:11:46.99938Z","shell.execute_reply.started":"2022-11-25T21:11:06.66222Z","shell.execute_reply":"2022-11-25T21:11:46.998297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ad_v3_roberta_large Sub","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, gc, re, warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import AutoModel,AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append('../input/iterative-stratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ntrain_df = pd.read_csv(\"../input/feedback-prize-english-language-learning/train.csv\")\ntest_df = pd.read_csv(\"../input/feedback-prize-english-language-learning/test.csv\")\n\ntrain_df[\"src\"]=\"train\"\ntest_df[\"src\"]=\"test\"\n\ndf = pd.concat([train_df, test_df], ignore_index=True)\n\ntarget_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]\n\nFOLDS = 25\n\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\nfor i,(train_index, val_index) in enumerate(skf.split(train_df, train_df[target_cols])):\n    train_df.loc[val_index,'fold'] = i\n    \ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nclass EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_length, tokenizer):\n        self.df = df.reset_index(drop=True)\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        text = self.df.loc[idx,\"full_text\"]\n        tokens = self.tokenizer(\n                        text,\n                        None,\n                        add_special_tokens=True,\n                        padding='max_length',\n                        truncation=True,\n                        max_length=self.max_length,\n                        return_tensors=\"pt\")\n        \n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens\n    \n    \ndef get_embeddings(df, model_name, max_length, batch_size, device, verbose=True):\n    model = AutoModel.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    dataset = EmbedDataset(df, max_length, tokenizer)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n    \n    model = model.to(device)\n    model.eval()\n    all_train_text_feats = []\n    \n    for batch in tqdm(dataloader, total=len(dataloader)):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        \n        with torch.no_grad():\n            model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        all_train_text_feats.extend(sentence_embeddings)\n        \n    all_train_text_feats = np.array(all_train_text_feats)\n    if verbose:\n        print('Embeddings shape',all_train_text_feats.shape)\n        \n    return all_train_text_feats\n\n\nmodel_name = '/kaggle/input/all-datasets-v3-robertalarge/roberta-large '\n\nwith open(os.path.join('../input/fb3-embeddings/ad_v3_roberta_large.npy'), 'rb') as file:\n    fb_bart_large_embeddings = np.load(file)\n    \nfb_bart_large_embeddings_test = get_embeddings(test_df, model_name, max_length=512, batch_size=4, device='cuda',)\n\n\n\nall_train_text_feats = np.concatenate([\n    fb_bart_large_embeddings,\n],axis=1)\n\n\nte_text_feats = np.concatenate([\n    fb_bart_large_embeddings_test,\n],axis=1)\n\n\ngc.collect()\nprint('Our concatenated embeddings have shape', all_train_text_feats.shape )\n\nfrom cuml.svm import SVR\nimport cuml\nprint('RAPIDS version',cuml.__version__)\n\nfrom sklearn.metrics import mean_squared_error\n\npreds = []\nscores = []\ndef comp_score(y_true,y_pred):\n    rmse_scores = []\n    for i in range(len(target_cols)):\n        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n    return np.mean(rmse_scores)\n\nfor fold in tqdm(range(FOLDS),total=FOLDS):\n# for fold in range(FOLDS):\n#     print('# =========================== Fold',fold+1, ' ===========================')\n    \n    dftr_ = train_df[train_df[\"fold\"]!=fold]\n    dfev_ = train_df[train_df[\"fold\"]==fold]\n    \n    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n    \n    ev_preds = np.zeros((len(ev_text_feats),6))\n    test_preds = np.zeros((len(te_text_feats),6))\n    for i,t in enumerate(target_cols):\n        clf = SVR(C=1)\n        clf.fit(tr_text_feats, dftr_[t].values)\n        ev_preds[:,i] = clf.predict(ev_text_feats)\n        test_preds[:,i] = clf.predict(te_text_feats)\n        \n    score = comp_score(dfev_[target_cols].values,ev_preds)\n    scores.append(score)\n#     print(\"Fold : {} RSME score: {}\".format(fold,score))\n    preds.append(test_preds)\n    \n\nprint('\\nOverall CV RSME =',np.mean(scores))\n\nsub = test_df.copy()\n\nsub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\nsub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\nsub = sub[sub_columns]\nsub.to_csv(\"submission_ad_v3_roberta_large.csv\",index=None)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:11:47.001195Z","iopub.execute_input":"2022-11-25T21:11:47.001887Z","iopub.status.idle":"2022-11-25T21:12:27.626069Z","shell.execute_reply.started":"2022-11-25T21:11:47.001847Z","shell.execute_reply":"2022-11-25T21:12:27.622772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### fb_bart_large_mnli Sub","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, gc, re, warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import AutoModel,AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append('../input/iterative-stratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ntrain_df = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/test.csv\")\n\ntrain_df[\"src\"]=\"train\"\ntest_df[\"src\"]=\"test\"\n\ndf = pd.concat([train_df, test_df], ignore_index=True)\n\ntarget_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]\n\nFOLDS = 25\n\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\nfor i,(train_index, val_index) in enumerate(skf.split(train_df, train_df[target_cols])):\n    train_df.loc[val_index,'fold'] = i\n    \ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nclass EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_length, tokenizer):\n        self.df = df.reset_index(drop=True)\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        text = self.df.loc[idx,\"full_text\"]\n        tokens = self.tokenizer(\n                        text,\n                        None,\n                        add_special_tokens=True,\n                        padding='max_length',\n                        truncation=True,\n                        max_length=self.max_length,\n                        return_tensors=\"pt\")\n        \n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens\n    \n    \ndef get_embeddings(df, model_name, max_length, batch_size, device, verbose=True):\n    model = AutoModel.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    dataset = EmbedDataset(df, max_length, tokenizer)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n    \n    model = model.to(device)\n    model.eval()\n    all_train_text_feats = []\n    \n    for batch in tqdm(dataloader, total=len(dataloader)):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        \n        with torch.no_grad():\n            model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        all_train_text_feats.extend(sentence_embeddings)\n        \n    all_train_text_feats = np.array(all_train_text_feats)\n    if verbose:\n        print('Embeddings shape',all_train_text_feats.shape)\n        \n    return all_train_text_feats\n\n\nmodel_name = '/kaggle/input/facebook/bart-large-mnli'\n\nwith open(os.path.join('/kaggle/input/fb3-embeddings/fb_bart_large_mnli.npy'), 'rb') as file:\n    fb_bart_large_embeddings = np.load(file)\n    \nfb_bart_large_embeddings_test = get_embeddings(test_df, model_name, max_length=512, batch_size=4, device='cuda',)\n\n\n\nall_train_text_feats = np.concatenate([\n    fb_bart_large_embeddings,\n],axis=1)\n\n\nte_text_feats = np.concatenate([\n    fb_bart_large_embeddings_test,\n],axis=1)\n\n\ngc.collect()\nprint('Our concatenated embeddings have shape', all_train_text_feats.shape )\n\nfrom cuml.svm import SVR\nimport cuml\nprint('RAPIDS version',cuml.__version__)\n\nfrom sklearn.metrics import mean_squared_error\n\npreds = []\nscores = []\ndef comp_score(y_true,y_pred):\n    rmse_scores = []\n    for i in range(len(target_cols)):\n        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n    return np.mean(rmse_scores)\n\nfor fold in tqdm(range(FOLDS),total=FOLDS):\n# for fold in range(FOLDS):\n#     print('# =========================== Fold',fold+1, ' ===========================')\n    \n    dftr_ = train_df[train_df[\"fold\"]!=fold]\n    dfev_ = train_df[train_df[\"fold\"]==fold]\n    \n    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n    \n    ev_preds = np.zeros((len(ev_text_feats),6))\n    test_preds = np.zeros((len(te_text_feats),6))\n    for i,t in enumerate(target_cols):\n        clf = SVR(C=1)\n        clf.fit(tr_text_feats, dftr_[t].values)\n        ev_preds[:,i] = clf.predict(ev_text_feats)\n        test_preds[:,i] = clf.predict(te_text_feats)\n        \n    score = comp_score(dfev_[target_cols].values,ev_preds)\n    scores.append(score)\n#     print(\"Fold : {} RSME score: {}\".format(fold,score))\n    preds.append(test_preds)\n    \n\nprint('\\nOverall CV RSME =',np.mean(scores))\n\nsub = test_df.copy()\n\nsub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\nsub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\nsub = sub[sub_columns]\nsub.to_csv(\"submission_fb_bart_large_mnli.csv\",index=None)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:12:27.628346Z","iopub.execute_input":"2022-11-25T21:12:27.628905Z","iopub.status.idle":"2022-11-25T21:13:05.94063Z","shell.execute_reply.started":"2022-11-25T21:12:27.628855Z","shell.execute_reply":"2022-11-25T21:13:05.939413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### all-roberta-large-v1","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os, gc, re, warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import AutoModel,AutoTokenizer\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append('../input/iterative-stratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\ntrain_df = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/feedback-prize-english-language-learning/test.csv\")\n\ntrain_df[\"src\"]=\"train\"\ntest_df[\"src\"]=\"test\"\n\ndf = pd.concat([train_df, test_df], ignore_index=True)\n\ntarget_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions',]\n\nFOLDS = 25\n\nskf = MultilabelStratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\nfor i,(train_index, val_index) in enumerate(skf.split(train_df, train_df[target_cols])):\n    train_df.loc[val_index,'fold'] = i\n    \ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output.last_hidden_state.detach().cpu()\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n        input_mask_expanded.sum(1), min=1e-9\n    )\n\n\nclass EmbedDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_length, tokenizer):\n        self.df = df.reset_index(drop=True)\n        self.max_length = max_length\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        text = self.df.loc[idx,\"full_text\"]\n        tokens = self.tokenizer(\n                        text,\n                        None,\n                        add_special_tokens=True,\n                        padding='max_length',\n                        truncation=True,\n                        max_length=self.max_length,\n                        return_tensors=\"pt\")\n        \n        tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n        return tokens\n    \n    \ndef get_embeddings(df, model_name, max_length, batch_size, device, verbose=True):\n    model = AutoModel.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    dataset = EmbedDataset(df, max_length, tokenizer)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=False)\n    \n    model = model.to(device)\n    model.eval()\n    all_train_text_feats = []\n    \n    for batch in tqdm(dataloader, total=len(dataloader)):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        \n        with torch.no_grad():\n            model_output = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n        sentence_embeddings = mean_pooling(model_output, attention_mask.detach().cpu())\n        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n        sentence_embeddings =  sentence_embeddings.squeeze(0).detach().cpu().numpy()\n        all_train_text_feats.extend(sentence_embeddings)\n        \n    all_train_text_feats = np.array(all_train_text_feats)\n    if verbose:\n        print('Embeddings shape',all_train_text_feats.shape)\n        \n    return all_train_text_feats\n\n\nmodel_name = '/kaggle/input/all-roberta-large-v1/all_roberta_large_v1'\n\nwith open(os.path.join('/kaggle/input/fb3-embeddings/all_roberta_large_v1.npy'), 'rb') as file:\n    fb_bart_large_embeddings = np.load(file)\n    \nfb_bart_large_embeddings_test = get_embeddings(test_df, model_name, max_length=512, batch_size=4, device='cuda',)\n\n\n\nall_train_text_feats = np.concatenate([\n    fb_bart_large_embeddings,\n],axis=1)\n\n\nte_text_feats = np.concatenate([\n    fb_bart_large_embeddings_test,\n],axis=1)\n\n\ngc.collect()\nprint('Our concatenated embeddings have shape', all_train_text_feats.shape )\n\nfrom cuml.svm import SVR\nimport cuml\nprint('RAPIDS version',cuml.__version__)\n\nfrom sklearn.metrics import mean_squared_error\n\npreds = []\nscores = []\ndef comp_score(y_true,y_pred):\n    rmse_scores = []\n    for i in range(len(target_cols)):\n        rmse_scores.append(np.sqrt(mean_squared_error(y_true[:,i],y_pred[:,i])))\n    return np.mean(rmse_scores)\n\nfor fold in tqdm(range(FOLDS),total=FOLDS):\n# for fold in range(FOLDS):\n#     print('# =========================== Fold',fold+1, ' ===========================')\n    \n    dftr_ = train_df[train_df[\"fold\"]!=fold]\n    dfev_ = train_df[train_df[\"fold\"]==fold]\n    \n    tr_text_feats = all_train_text_feats[list(dftr_.index),:]\n    ev_text_feats = all_train_text_feats[list(dfev_.index),:]\n    \n    ev_preds = np.zeros((len(ev_text_feats),6))\n    test_preds = np.zeros((len(te_text_feats),6))\n    for i,t in enumerate(target_cols):\n        clf = SVR(C=1)\n        clf.fit(tr_text_feats, dftr_[t].values)\n        ev_preds[:,i] = clf.predict(ev_text_feats)\n        test_preds[:,i] = clf.predict(te_text_feats)\n        \n    score = comp_score(dfev_[target_cols].values,ev_preds)\n    scores.append(score)\n#     print(\"Fold : {} RSME score: {}\".format(fold,score))\n    preds.append(test_preds)\n    \n\nprint('\\nOverall CV RSME =',np.mean(scores))\n\nsub = test_df.copy()\n\nsub.loc[:,target_cols] = np.average(np.array(preds),axis=0) #,weights=[1/s for s in scores]\nsub_columns = pd.read_csv(\"../input/feedback-prize-english-language-learning/sample_submission.csv\").columns\nsub = sub[sub_columns]\nsub.to_csv(\"submission_all_roberta_large_v1.csv\",index=None)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-28T10:20:56.036296Z","iopub.execute_input":"2022-11-28T10:20:56.036716Z","iopub.status.idle":"2022-11-28T10:21:46.053668Z","shell.execute_reply.started":"2022-11-28T10:20:56.036683Z","shell.execute_reply":"2022-11-28T10:21:46.052599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submission","metadata":{}},{"cell_type":"code","source":"cohesion_params = {\n'w1': 0.0,\n'w2': 0.582090873995617,\n'w3': 0.08868670946014108,\n'w4': 0.0,\n'w5': 0.14256297158423278,\n'w6': 0.0038073273644528005,\n'w7': 0.24051324161351678,\n'w8': 0.0,\n'w9': 0.0,\n'w10': 0.7862496103372187,\n'w11': 0.0,\n'w12': 0.2681822633546716,\n'w13': 0.0,\n'w14': 0.0,\n'w15': 0.28655998078690104, \n'w16': 0.0,\n'w17': 0.13052768112895427,\n'w18': 0.2576961605746973,\n'w19': 0.01791441665701174,\n'w20': 0.5\n}\n\n\nsyntax_params = {\n'w1': 0.0,\n'w2': 0.8466339874016717,\n'w3': 0.24699160902252312,\n'w4': 0.45172032437366627,\n'w5': 0.20230744199065628,\n'w6': 0.08572055526000201,\n'w7': 0.15072082833029135,\n'w8': 0.11290609048162618,\n'w9': 0.17578301415435557,\n'w10': 0.0,\n'w11': 0.21168366686691406,\n'w12': 0.3072189141092456,\n'w13': 0.13570732112856576,\n'w14': 0.0,\n'w15': 0.0, \n'w16': 0.0,\n'w17': 0.05030880871152592,\n'w18': 0.0,\n'w19': 0.0043819554664268415,\n'w20': 0.5\n}\n\n\nvocabulary_params = {\n'w1': 0.0,\n'w2': 0.9806452939462846,\n'w3': 0.2723605069766004,\n'w4': 0.09320920775166273,\n'w5': 0.15775977843609892,\n'w6': 0.0,\n'w7': 0.28650798741718775,\n'w8': 0.0,\n'w9': 0.0,\n'w10': 0.0,\n'w11': 0.8348332049215981,\n'w12': 0.46847877776107516,\n'w13': 0.0,\n'w14': 0.0,\n'w15': 0.0, \n'w16': 0.21262565761978575,\n'w17': 0.0,\n'w18': 0.4149318319469712,\n'w19': 0.0,\n'w20': 0.5\n}\n\n\nphraseology_params = {\n'w1': 0.4185029337443942,\n'w2': 0.31017696697752817,\n'w3': 0.3490430198213265,\n'w4': 0.1684639587018389,\n'w5': 0.14104578942538834,\n'w6': 0.0,\n'w7': 0.25846087373557347,\n'w8': 0.10550665830893159,\n'w9': 0.16239205775383195,\n'w10': 0.0,\n'w11': 0.6032314132416007,\n'w12': 0.08037899037994861,\n'w13': 0.1066308024357757,\n'w14': 0.0,\n'w15': 0.0057755331819348155,\n'w16': 0.0,\n'w17': 0.08670696928285031,\n'w18': 0.11775531837030963,\n'w19': 0.008131936097102142,\n'w20': 0.5\n}\n\n\n\ngrammar_params = {\n'w1': 0.13881704283983315,\n'w2': 0.6868627209462645,\n'w3': 0.41919000155474695,\n'w4': 0.0,\n'w5': 0.0,\n'w6': 0.0,\n'w7': 0.1733616109624987,\n'w8': 0.17540674579032667,\n'w9':  0.0,\n'w10': 0.45361409581385786,\n'w11': 0.9999963915756588,\n'w12': 0.2560889054054799,\n'w13': 0.0,\n'w14': 0.5504465764503463,\n'w15': 0.9322318619811504,  \n'w16': 0.0,\n'w17': 0.0,\n'w18': 0.0,\n'w19': 0.0,\n'w20': 0.5\n}\n\n\n\nconventions_params = {\n'w1': 0.5519369189067282,\n'w2': 0.7761298427228378,\n'w3': 0.0,\n'w4': 0.09113458324323657,\n'w5': 0.09573713242683393,\n'w6': 0.10942092087350164,\n'w7': 0.007929047182409905,\n'w8': 0.0,\n'w9':  0.0,\n'w10': 0.44350363206724763,\n'w11': 0.20780569898513726,\n'w12': 0.09938029141537365,\n'w13': 0.23671579507997162,\n'w14': 0.0,\n'w15': 0.0,  \n'w16': 0.03204307159190761,\n'w17': 0.15707093503412223,\n'w18': 0.0,\n'w19': 0.0,\n'w20': 0.5\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:13:05.942793Z","iopub.execute_input":"2022-11-25T21:13:05.943203Z","iopub.status.idle":"2022-11-25T21:13:05.961091Z","shell.execute_reply.started":"2022-11-25T21:13:05.943161Z","shell.execute_reply":"2022-11-25T21:13:05.959891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_weights = {\n    \n'cohesion': {\n     'exp01_fb3_part2': cohesion_params['w1'],\n     'model23': cohesion_params['w2'],\n     '0926-deberta-v3-large': cohesion_params['w3'],\n     'nischay': cohesion_params['w4'],\n     '0919-deberta-v2-xlarge-mnli': cohesion_params['w5'],\n     'exp14_fb3_roberta': cohesion_params['w6'],\n     '0911-deberta-v3-large': cohesion_params['w7'],\n     'exp13_fb3': cohesion_params['w8'],\n     'exp02_fb3_part2_distilbert': cohesion_params['w9'],\n     'model52': cohesion_params['w10'],\n     'exp11_fb3_pl_train': cohesion_params['w11'],\n     'exp11_fb3_rlarge': cohesion_params['w12'],\n     'model68': cohesion_params['w13'],\n     'model70': cohesion_params['w14'],\n     'model71': cohesion_params['w15'],\n     'fb_bart_large': cohesion_params['w16'],\n     'ad_v3_roberta_large': cohesion_params['w17'],\n     'fb_bart_large_mnli': cohesion_params['w18'],\n     'all_roberta_large_v1': cohesion_params['w19'],\n#      'exp12_fb3': conventions_params['w20'],\n    \n#      'model75': cohesion_params['w19'],\n     'model55': conventions_params['w20'],\n        \n },\n        \n'syntax': {\n     'exp01_fb3_part2': syntax_params['w1'],\n     'model23': syntax_params['w2'],\n     '0926-deberta-v3-large': syntax_params['w3'],\n     'nischay': syntax_params['w4'],\n     '0919-deberta-v2-xlarge-mnli': syntax_params['w5'],\n     'exp14_fb3_roberta': syntax_params['w6'],\n     '0911-deberta-v3-large': syntax_params['w7'],\n     'exp13_fb3': syntax_params['w8'],\n     'exp02_fb3_part2_distilbert': syntax_params['w9'],\n     'model52': syntax_params['w10'],\n     'exp11_fb3_pl_train': syntax_params['w11'],\n     'exp11_fb3_rlarge': syntax_params['w12'],\n     'model68': syntax_params['w13'],\n     'model70': syntax_params['w14'],\n     'model71': syntax_params['w15'],\n     'fb_bart_large': syntax_params['w16'],\n     'ad_v3_roberta_large': syntax_params['w17'],\n     'fb_bart_large_mnli': syntax_params['w18'],\n    \n     'all_roberta_large_v1': syntax_params['w19'],\n#      'exp12_fb3': conventions_params['w20'], \n    \n# #      'model75': syntax_params['w19'],\n     'model55': conventions_params['w20'],\n },\n    \n'vocabulary': {\n     'exp01_fb3_part2': vocabulary_params['w1'],\n     'model23': vocabulary_params['w2'],\n     '0926-deberta-v3-large': vocabulary_params['w3'],\n     'nischay': vocabulary_params['w4'],\n     '0919-deberta-v2-xlarge-mnli': vocabulary_params['w5'],\n     'exp14_fb3_roberta': vocabulary_params['w6'],\n     '0911-deberta-v3-large': vocabulary_params['w7'],\n     'exp13_fb3': vocabulary_params['w8'],\n     'exp02_fb3_part2_distilbert': vocabulary_params['w9'],\n     'model52': vocabulary_params['w10'],\n     'exp11_fb3_pl_train': vocabulary_params['w11'],\n     'exp11_fb3_rlarge': vocabulary_params['w12'],\n     'model68': vocabulary_params['w13'],\n     'model70': vocabulary_params['w14'],\n     'model71': vocabulary_params['w15'],\n     'fb_bart_large': vocabulary_params['w16'],\n     'ad_v3_roberta_large': vocabulary_params['w17'],\n     'fb_bart_large_mnli': vocabulary_params['w18'],\n    \n     'all_roberta_large_v1': vocabulary_params['w19'],\n#      'exp12_fb3': conventions_params['w20'],\n    \n# #      'model75': vocabulary_params['w19'],\n     'model55': conventions_params['w20'],\n }, \n    \n'phraseology': {\n     'exp01_fb3_part2': phraseology_params['w1'],\n     'model23': phraseology_params['w2'],\n     '0926-deberta-v3-large': phraseology_params['w3'],\n     'nischay': phraseology_params['w4'],\n     '0919-deberta-v2-xlarge-mnli': phraseology_params['w5'],\n     'exp14_fb3_roberta': phraseology_params['w6'],\n     '0911-deberta-v3-large': phraseology_params['w7'],\n     'exp13_fb3': phraseology_params['w8'],\n     'exp02_fb3_part2_distilbert': phraseology_params['w9'],\n     'model52': phraseology_params['w10'],\n     'exp11_fb3_pl_train': phraseology_params['w11'],\n     'exp11_fb3_rlarge': phraseology_params['w12'],\n     'model68': phraseology_params['w13'],\n     'model70': phraseology_params['w14'],\n     'model71': phraseology_params['w15'],\n     'fb_bart_large': phraseology_params['w16'],\n     'ad_v3_roberta_large': phraseology_params['w17'],\n     'fb_bart_large_mnli': phraseology_params['w18'],\n    \n     'all_roberta_large_v1': phraseology_params['w19'],\n#      'exp12_fb3': conventions_params['w20'],\n    \n# #      'model75': phraseology_params['w19'],\n     'model55': conventions_params['w20'],\n },  \n    \n'grammar': {\n     'exp01_fb3_part2': grammar_params['w1'],\n     'model23': grammar_params['w2'],\n     '0926-deberta-v3-large': grammar_params['w3'],\n     'nischay': grammar_params['w4'],\n     '0919-deberta-v2-xlarge-mnli': grammar_params['w5'],\n     'exp14_fb3_roberta': grammar_params['w6'],\n     '0911-deberta-v3-large': grammar_params['w7'],\n     'exp13_fb3': grammar_params['w8'],\n     'exp02_fb3_part2_distilbert': grammar_params['w9'],\n     'model52': grammar_params['w10'],\n     'exp11_fb3_pl_train': grammar_params['w11'],\n     'exp11_fb3_rlarge': grammar_params['w12'],\n     'model68': grammar_params['w13'],\n     'model70': grammar_params['w14'],\n     'model71': grammar_params['w15'],\n     'fb_bart_large': grammar_params['w16'],\n     'ad_v3_roberta_large': grammar_params['w17'],\n     'fb_bart_large_mnli': grammar_params['w18'],\n    \n     'all_roberta_large_v1': grammar_params['w19'],\n#      'exp12_fb3': conventions_params['w20'],\n    \n# #      'model75': grammar_params['w19'],\n     'model55': conventions_params['w20'],\n },\n    \n'conventions': {\n     'exp01_fb3_part2': conventions_params['w1'],\n     'model23': conventions_params['w2'],\n     '0926-deberta-v3-large': conventions_params['w3'],\n     'nischay': conventions_params['w4'],\n     '0919-deberta-v2-xlarge-mnli': conventions_params['w5'],\n     'exp14_fb3_roberta': conventions_params['w6'],\n     '0911-deberta-v3-large': conventions_params['w7'],\n     'exp13_fb3': conventions_params['w8'],\n     'exp02_fb3_part2_distilbert': conventions_params['w9'],\n     'model52': conventions_params['w10'],\n     'exp11_fb3_pl_train': conventions_params['w11'],\n     'exp11_fb3_rlarge': conventions_params['w12'],\n     'model68': conventions_params['w13'],\n     'model70': conventions_params['w14'],\n     'model71': conventions_params['w15'],\n     'fb_bart_large': conventions_params['w16'],\n     'ad_v3_roberta_large': conventions_params['w17'],\n     'fb_bart_large_mnli': conventions_params['w18'],\n    \n     'all_roberta_large_v1': conventions_params['w19'],\n#      'exp12_fb3': conventions_params['w20'],\n    \n# #      'model75': conventions_params['w19'],\n     'model55': conventions_params['w20'],\n    \n}     \n}   \n","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:13:05.963074Z","iopub.execute_input":"2022-11-25T21:13:05.963441Z","iopub.status.idle":"2022-11-25T21:13:05.98569Z","shell.execute_reply.started":"2022-11-25T21:13:05.963405Z","shell.execute_reply":"2022-11-25T21:13:05.984567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_names = list(best_weights['cohesion'].keys())\nsubmission = pd.read_csv('../input/feedback-prize-english-language-learning/sample_submission.csv')\nsubmission = submission[['text_id']]\n\nfor model in model_names:\n    df = pd.read_csv(f'./submission_{model}.csv')\n    if 'full_text' in df.columns:\n        df.drop('full_text', axis=1, inplace=True)\n    df.columns = ['text_id'] + [col+'_'+model for col in df.columns if col != 'text_id']\n    submission = pd.merge(submission, df, on='text_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:13:05.988601Z","iopub.execute_input":"2022-11-25T21:13:05.989005Z","iopub.status.idle":"2022-11-25T21:13:06.100447Z","shell.execute_reply.started":"2022-11-25T21:13:05.988968Z","shell.execute_reply":"2022-11-25T21:13:06.099494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_columns = ['cohesion', 'syntax', 'vocabulary',\n                  'phraseology', 'grammar', 'conventions']\n\nsubmission[target_columns] = 0\n\nfor col in target_columns:\n    w_ = best_weights[col]\n    for fn, w in w_.items():\n        submission[col] += submission[col+'_'+fn] * w\n        \n    submission[col] = submission[col]/np.sum(list(w_.values()))\n    \nsubmission = submission[['text_id'] + target_columns]\nsubmission[target_columns] = submission[target_columns].clip(1, 5)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-25T21:13:06.101998Z","iopub.execute_input":"2022-11-25T21:13:06.102345Z","iopub.status.idle":"2022-11-25T21:13:06.169433Z","shell.execute_reply.started":"2022-11-25T21:13:06.102309Z","shell.execute_reply":"2022-11-25T21:13:06.168511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-25T06:26:37.64461Z","iopub.execute_input":"2022-11-25T06:26:37.644948Z","iopub.status.idle":"2022-11-25T06:26:37.657666Z","shell.execute_reply.started":"2022-11-25T06:26:37.644914Z","shell.execute_reply":"2022-11-25T06:26:37.656581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}